{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page 1\n",
      "Getting page 2\n",
      "Getting page 3\n",
      "Getting page 4\n",
      "Getting page 5\n",
      "Getting page 6\n",
      "Getting page 7\n",
      "Getting page 8\n",
      "Getting page 9\n",
      "Getting page 10\n",
      "Getting page 11\n",
      "Getting page 12\n",
      "Getting page 13\n",
      "Getting page 14\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "auth=('admin','supersecret123')\n",
    "\n",
    "r = requests.get(\"https://harrigt.papro.org.uk/api/news?hidden=true\", auth=auth)\n",
    "result = r.json()\n",
    "\n",
    "articles=[]\n",
    "for i in range(math.ceil(result['total_count']/10)):\n",
    "    print(\"Getting page {}\".format(i+1))\n",
    "    offset = i*10\n",
    "    \n",
    "    r = requests.get(\"https://harrigt.papro.org.uk/api/news\",\n",
    "                     params={\"hidden\":\"true\",\n",
    "                            \"offset\":offset},\n",
    "                     auth=auth)\n",
    "    articles.extend(r.json()['articles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page 1\n",
      "Getting page 2\n",
      "Getting page 3\n",
      "Getting page 4\n",
      "Getting page 5\n",
      "Getting page 6\n",
      "Getting page 7\n",
      "Getting page 8\n",
      "Getting page 9\n",
      "Getting page 10\n",
      "Getting page 11\n",
      "Getting page 12\n",
      "Getting page 13\n",
      "Getting page 14\n",
      "Getting page 15\n",
      "Getting page 16\n",
      "Getting page 17\n",
      "Getting page 18\n",
      "Getting page 19\n",
      "Getting page 20\n",
      "Getting page 21\n",
      "Getting page 22\n",
      "Getting page 23\n",
      "Getting page 24\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "r = requests.get(\"https://harrigt.papro.org.uk/api/news?spam=true\", auth=auth)\n",
    "result = r.json()\n",
    "\n",
    "spam=[]\n",
    "for i in range(math.ceil(result['total_count']/10)):\n",
    "    print(\"Getting page {}\".format(i+1))\n",
    "    offset = i*10\n",
    "    \n",
    "    r = requests.get(\"https://harrigt.papro.org.uk/api/news\",\n",
    "                     params={\"spam\":\"true\",\n",
    "                            \"offset\":offset},\n",
    "                     auth=auth)\n",
    "    spam.extend(r.json()['articles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page 1\n",
      "Getting page 2\n",
      "Getting page 3\n",
      "Getting page 4\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "r = requests.get(\"https://harrigt.papro.org.uk/api/news?linked=true\", auth=auth)\n",
    "result = r.json()\n",
    "\n",
    "linked=[]\n",
    "for i in range(math.ceil(result['total_count']/10)):\n",
    "    print(\"Getting page {}\".format(i+1))\n",
    "    offset = i*10\n",
    "    \n",
    "    r = requests.get(\"https://harrigt.papro.org.uk/api/news\",\n",
    "                     params={\"linked\":\"true\",\n",
    "                            \"offset\":offset},\n",
    "                     auth=auth)\n",
    "    linked.extend(r.json()['articles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "\n",
    "X = vect.fit_transform( [ article['content'] for article in (articles + linked + spam) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = [\"ham\"] * (len(articles) + len(linked)) + ['spam'] * len(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74689826302729534"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "predicted = cross_val_predict(clf, X.toarray(), y)\n",
    "accuracy_score(predicted, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__min_df':(0, 0.05, 0.1),\n",
    "    'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__C' : [1, 2, 5, 10, 20, 100],\n",
    "    'clf__kernel' : ['linear', 'rbf']\n",
    "    #'clf__n_estimators': [10,20,50,100]\n",
    "}\n",
    "\n",
    "cv_splits = 5\n",
    "num_threads=-1\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', svm.SVC(C=1, probability=True)),\n",
    "    #('clf', ExtraTreesClassifier())\n",
    "])\n",
    "\n",
    "clf = GridSearchCV(pipeline, parameters, verbose=1, n_jobs=num_threads, cv=cv_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [ article['content'] for article in (articles + linked + spam) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 1,\n",
       " 'clf__kernel': 'linear',\n",
       " 'tfidf__norm': 'l2',\n",
       " 'vect__max_df': 0.75,\n",
       " 'vect__max_features': 10000,\n",
       " 'vect__min_df': 0,\n",
       " 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(x,y)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('vect', CountVectorizer(analyzer=stemmed_words)),\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', svm.SVC()),\n",
    "    #('clf', ExtraTreesClassifier())\n",
    "])\n",
    "\n",
    "params = {'clf__C': 1,\n",
    " 'clf__kernel': 'linear',\n",
    " 'clf__probability' : True,\n",
    " 'tfidf__norm': 'l2',\n",
    " 'vect__max_df': 0.75,\n",
    " 'vect__max_features': 10000,\n",
    " 'vect__min_df': 0,\n",
    " 'vect__ngram_range': (1, 2)}\n",
    "\n",
    "pipeline.set_params(**params)\n",
    "\n",
    "#y_pred = pipeline.predict(y_test)\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "\n",
    "y_pred = cross_val_predict(pipeline, X,y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.88      0.82      0.85       169\n",
      "       spam       0.87      0.92      0.90       234\n",
      "\n",
      "avg / total       0.88      0.88      0.88       403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "print(classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pipeline.fit(X,y)\n",
    "\n",
    "with open(\"spam_classifier.pickle\", \"wb\") as f:\n",
    "    pickle.dump(pipeline, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = pickle.load(open(\"spam_classifier.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00283384,  0.99716616]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.predict_proba([\"\"\"\"\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'],\n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
